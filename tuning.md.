Then access Grafana dashboard on your local browser on URL:  http://localhost:3000 

Default Logins are:

Username: admin
Password: admin

You’re required to change the password on first login:

In the scrape_configs part you need to check configuration for Jenkins server
Check it: go to http://localhost:9090/targets


[https://souravatta.medium.com/monitor-jenkins-build-logs-using-elk-stack-697e13b78cb]

Configuring Filebeat

Filebeat is a lightweight logs shipper. It is installed as an agent on your servers (i.e. Jenkins server) which will monitor the Jenkins log file, collect events, and ships to Logstash for parsing.

If you want to want to monitor all the Jenkins instances, then you need to install filebeat in all the instances and ships the application log to logstash. Here, we will ship the application logs of one Jenkins instance only.

Below is the filebeat.ymlto monitor jenkins log file.

    Note: Filebeat will talk to Logstash on port 5045 (you can given port), which should be same will configuring Logstash.

################## Filebeat Configuration Example ###################======================== Filebeat inputs ==========================filebeat.inputs:
- type: log
  enabled: true
  paths:
   - /var/log/jenkins/jenkins.log
  exclude_files: ['.gz$']
  multiline.pattern: '^[a-zA-Z]+\s[0-9]{1,2},\s[0-9]{4}\s[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\s(?:AM|am|PM|pm)'
  multiline.negate: true
  multiline.match: after
  fields:
    type: jenkins-server
  fields_under_root: true#========================== Outputs ================================#------------------------- Logstash output -------------------------
output.logstash:
  # The Logstash hosts
  hosts: ["10.165.95.29:5045"]
  bulk_max_size: 200#======================== Processors ==============================# Configure processors to enhance or manipulate events generated by the beat.processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~

Configuring Logstash

Logstash process events in three stages: input → filter → output. In this case,

    input: get logs data from filebeat
    filter: used grok, date, and mutate filter plugins to filter and process logs
    output: store the processed logs in elasticsearch

input {
 beats {
        port => "5045"
    }
  }filter {
  if [type] == "jenkins-server" {# set all messages from the jenkins log as type 'jenkins' and add the @message field.
          mutate {
              add_field => ["@message_type", "jenkins"]
              add_field => ["@message", "%{message}"]
          }}
  }# now that we have possibly-multiline events, we can clean them up.
  filter {# munge the possibly-multiline messages into a single string
      mutate {
          join => ["@message", "\n"]
      }# split @message into __date and __msg, and overwrite the @timestamp value.
      grok {
          match => [ "@message", "^(?<__date>%{MONTH} %{MONTHDAY}, %{YEAR} %{TIME} (AM|PM)) (?<__msg>.+)" ]
      }
      date {
          match  => [ "__date", "MMM dd, YYYY HH:mm:ss a"]
      }# ...now some patterns to categorize specific event types...# parse build completion messages, adding the jenkins_* fields and the 'build' tag
      grok {
          match => [ "@message", "(?<jenkins_job>\S+) #(?<jenkins_build_number>\d+) (?<__msg>.+): (?<jenkins_build_status>\w+)" ]
          tag_on_failure => []
          overwrite => true
          add_tag => ['build']
      }
       
   # convert build number from string to integer
   mutate {
                convert => ["jenkins_build_number", "integer"]
                }# tag messages that come from the perforce SCM plugin (and associated classes)
      grok {
          match => [ "@message", "\.perforce\."]
          tag_on_failure => []
          add_tag => ['p4-plugin']
      }# if we have extracted a short message string, replace @message with it now
      if [__msg] {
          mutate {
              replace => ["@message","%{__msg}"]
          }
      }# convert @message back into an array of lines
      mutate {
          split => ["@message", "\n"]
      }
  }# clean-up temporary fields and unwanted tags.
  filter {
      mutate {
          remove_field => [
              "message",
              "__msg",
              "__date",
              "dumps1",
              "plugin_command"
          ]
          remove_tag => [
              "multiline",
              "_grokparsefailure"
          ]
      }
  }# send it on to the elasticsearch
  output {
    elasticsearch {
                hosts => ["10.165.35.19:9200"]
    
    # username & password to connect to elaticsearch
                user => "elastic"
                password => "elastic"
    
                action => "index"
                index => "jenkins-%{+YYYY.MM.dd}"}
 
 # use this if you want to verify logs are being sent to elasticsearch or not
 
        #stdout { codec => rubydebug }
  }

Configuring Elasticsearch

Elasticsearch will store and process the data in jenkins-%{+YYYY.MM.dd}the index. We will not change the default configuration of elasticsearch, which was configured during elasticsearch installation.
Configuring Kibana

Kibana is used to visualize and analyze the data stored in elasticsearch indices. We will use jenkins-%{+YYYY.MM.dd}the index to visualize the Jenkins build logs and create different fancy visualizations and combine visualizations in one dashboard. We need to create Index Pattern in Kibana first to create visualizations and dashboards.

Steps to create Index Pattern in Kibana:

    Login to Kibana (default: http://localhost:5601)
    Go to Settings → Kibana → Index Patterns
    Click on Create Index Pattern. Define an Index pattern (say jenkins-*)
    You will be able to see all the Jenkins indices will get listed. Then, click on the Next step.
    Choose the time filter (say @timestamp). Then, click on the Create index pattern.

Index Pattern jenkins-*

Now, we are ready to analyze, create a visualization, and a dashboard with captured data or events. Below we can see a sample visualization of SEVERE logs and a dashboard view of Success and Failure jobs, Frequently Running Jobs, and Jenkins All Jobs Status.

[https://www.elastic.co/guide/en/kibana/7.16/index-patterns.html]
Create an index pattern
edit

If you collected data using one of the Kibana ingest options, uploaded a file, or added sample data, you get an index pattern for free, and can start exploring your data. If you loaded your own data, follow these steps to create an index pattern.

    Open the main menu, then click to Stack Management > Index Patterns.

    Click Create index pattern.

    Create index pattern

    Start typing in the Index pattern field, and Kibana looks for the names of indices, data streams, and aliases that match your input.
        To match multiple sources, use a wildcard (*). For example, filebeat-* matches filebeat-apache-a, filebeat-apache-b, and so on.
        To match multiple single sources, enter their names, separated with a comma. Do not include a space after the comma. filebeat-a,filebeat-b matches two indices, but not match filebeat-c.
        To exclude a source, use a minus sign (-), for example, -test3.

    If Kibana detects an index with a timestamp, expand the Timestamp field menu, and then select the default field for filtering your data by time.
        If your index doesn’t have time-based data, choose I don’t want to use the time filter.
        If you don’t set a default time field, you can’t use global time filters on your dashboards. This is useful if you have multiple time fields and want to create dashboards that combine visualizations based on different timestamps.

    Click Create index pattern.

    Kibana is now configured to use your Elasticsearch data. When a new field is added to an index, the index pattern field list is updated the next time the index pattern is loaded, for example, when you load the page or move between Kibana apps.
    Select this index pattern when you search and visualize your data.

After installing Filebeat, you need to configure it. Open the filebeat.yml file located in your Filebeat installation directory, and replace the contents with the following lines. Make sure paths points to the example Apache log file, logstash-tutorial.log, that you downloaded earlier:


https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html
filebeat.inputs:
- type: log
  paths:
    - /path/to/file/logstash-tutorial.log 
output.logstash:
  hosts: ["localhost:5044"]
  
  
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  logstash index in elasticsearch:
  
  http://localhost:9200/_cat/indices
  
  http://192.168.49.2:30920/logstash/_search?pretty
  
  
  kubectl replace --force -f ./elk/metricbeat-demonset configmap.yaml -n logging
  
  
  
